"""
GitHubè¶‹åŠ¿çˆ¬è™«æ¨¡å—
"""
import requests
import time
import json
from typing import List, Optional, Tuple
from datetime import datetime
from pyquery import PyQuery as pq
import codecs


class GitHubTrendingScraper:
    """GitHubè¶‹åŠ¿çˆ¬è™«"""
    
    def __init__(self, timeout: int = 30, max_retries: int = 3):
        """
        åˆå§‹åŒ–çˆ¬è™«
        
        Args:
            timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        """
        self.timeout = timeout
        self.max_retries = max_retries
        self.base_url = "https://github.com/trending"
        
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Encoding': 'gzip,deflate,sdch',
            'Accept-Language': 'zh-CN,zh;q=0.8'
        }
    
    def scrape(self, language: str = "python", output_file: Optional[str] = None) -> Tuple[bool, List[dict]]:
        """
        çˆ¬å–GitHubè¶‹åŠ¿
        
        Args:
            language: ç¼–ç¨‹è¯­è¨€ï¼ˆå¦‚python, rustç­‰ï¼‰
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            (æ˜¯å¦æˆåŠŸ, é¡¹ç›®åˆ—è¡¨)
        """
        attempts = 0
        last_error = None
        
        while attempts < self.max_retries:
            try:
                print(f"ğŸŒ å°è¯•çˆ¬å–GitHub {language} è¶‹åŠ¿ (å°è¯• {attempts + 1}/{self.max_retries})...")
                
                # æ„å»ºURL
                url = f"{self.base_url}/{language}"
                if language == "all":
                    url = self.base_url
                
                # å‘é€è¯·æ±‚
                response = requests.get(
                    url, 
                    headers=self.headers, 
                    timeout=self.timeout
                )
                
                print(f"ğŸ“¡ å“åº”çŠ¶æ€ç : {response.status_code}")
                
                if response.status_code == 200:
                    projects = self.parse_response(response.content, language)
                    
                    if projects:
                        print(f"âœ… æˆåŠŸçˆ¬å– {len(projects)} ä¸ªé¡¹ç›®")
                        
                        # ä¿å­˜åˆ°æ–‡ä»¶ï¼ˆå¦‚æœéœ€è¦ï¼‰
                        if output_file:
                            self.save_projects(projects, output_file, language)
                        
                        return True, projects
                    else:
                        print("âš ï¸  çˆ¬å–æˆåŠŸä½†æœªè§£æåˆ°é¡¹ç›®")
                        return False, []
                else:
                    print(f"âŒ HTTPé”™è¯¯: {response.status_code}")
                    last_error = f"HTTP {response.status_code}"
                    
            except requests.exceptions.Timeout:
                print("â° è¯·æ±‚è¶…æ—¶")
                last_error = "Timeout"
            except requests.exceptions.ConnectionError:
                print("ğŸ”Œ è¿æ¥é”™è¯¯")
                last_error = "ConnectionError"
            except Exception as e:
                print(f"âŒ çˆ¬å–å¤±è´¥: {e}")
                last_error = str(e)
            
            attempts += 1
            
            if attempts < self.max_retries:
                wait_time = 2 ** attempts  # æŒ‡æ•°é€€é¿
                print(f"â±ï¸  ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                time.sleep(wait_time)
        
        print(f"âŒ æ‰€æœ‰å°è¯•å¤±è´¥: {last_error}")
        return False, []
    
    def parse_response(self, html_content: bytes, language: str) -> List[dict]:
        """
        è§£æHTMLå“åº”
        
        Args:
            html_content: HTMLå†…å®¹
            language: ç¼–ç¨‹è¯­è¨€
            
        Returns:
            é¡¹ç›®åˆ—è¡¨
        """
        try:
            d = pq(html_content)
            items = d('div.Box article.Box-row')
            
            projects = []
            for index, item in enumerate(items, start=1):
                i = pq(item)
                
                # æå–é¡¹ç›®ä¿¡æ¯
                title_element = i(".lh-condensed a")
                title = title_element.text().strip()
                
                # æå–URL
                relative_url = title_element.attr("href")
                url = f"https://github.com{relative_url}" if relative_url else ""
                
                # æå–æè¿°
                description = i("p.col-9").text().strip()
                
                # æå–é¢å¤–ä¿¡æ¯ï¼ˆstars, forksç­‰ï¼‰
                stars_text = i(f"span[aria-label*='star']").text().strip()
                forks_text = i(f"span[aria-label*='fork']").text().strip()
                stars_today_text = i("span.float-sm-right").text().strip()
                
                # å°è¯•æå–starsæ•°
                stars = self.extract_number(stars_text)
                forks = self.extract_number(forks_text)
                stars_today = self.extract_stars_today(stars_today_text)
                
                # æ„å»ºé¡¹ç›®å¯¹è±¡
                project = {
                    "index": index,
                    "title": title,
                    "description": description,
                    "url": url,
                    "language": language,
                    "stars": stars,
                    "forks": forks,
                    "stars_today": stars_today,
                    "full_title": title,
                    "scraped_at": datetime.now().isoformat()
                }
                
                projects.append(project)
            
            return projects
            
        except Exception as e:
            print(f"è§£æHTMLå¤±è´¥: {e}")
            return []
    
    def extract_number(self, text: str) -> Optional[int]:
        """ä»æ–‡æœ¬ä¸­æå–æ•°å­—"""
        if not text:
            return None
        
        # ç§»é™¤åƒåˆ†ä½é€—å·ï¼Œæå–æ•°å­—
        import re
        match = re.search(r'[\d,]+', text.replace(',', ''))
        if match:
            try:
                return int(match.group())
            except ValueError:
                return None
        return None
    
    def extract_stars_today(self, text: str) -> Optional[int]:
        """æå–ä»Šæ—¥starsæ•°"""
        if not text:
            return None
        
        import re
        match = re.search(r'(\d+)\s*stars today', text.lower())
        if match:
            try:
                return int(match.group(1))
            except ValueError:
                return None
        return None
    
    def save_projects(self, projects: List[dict], output_file: str, language: str):
        """
        ä¿å­˜é¡¹ç›®åˆ°æ–‡ä»¶
        
        Args:
            projects: é¡¹ç›®åˆ—è¡¨
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            language: ç¼–ç¨‹è¯­è¨€
        """
        try:
            # ä¿å­˜ä¸ºJSONæ ¼å¼
            json_file = output_file.replace('.txt', '.json')
            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump({
                    "language": language,
                    "scraped_at": datetime.now().isoformat(),
                    "total_projects": len(projects),
                    "projects": projects
                }, f, ensure_ascii=False, indent=2)
            
            # ä¿å­˜ä¸ºæ–‡æœ¬æ ¼å¼ï¼ˆå…¼å®¹åŸæœ‰æ ¼å¼ï¼‰
            with codecs.open(output_file, "w", "utf-8") as f:
                for project in projects:
                    f.write(f"{project['index']}. [{project['title']}]:{project['description']}({project['url']})\n")
            
            print(f"ğŸ’¾ é¡¹ç›®æ•°æ®å·²ä¿å­˜åˆ°: {output_file}")
            
        except Exception as e:
            print(f"ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")
    
    def test_connection(self) -> bool:
        """æµ‹è¯•GitHubè¿æ¥"""
        try:
            response = requests.get(self.base_url, headers=self.headers, timeout=10)
            return response.status_code == 200
        except:
            return False