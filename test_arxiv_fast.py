#!/usr/bin/env python3
"""
arXivå¿«é€Ÿæµ‹è¯• - çˆ¬å–å°‘é‡æ•°æ®å¹¶å‘é€
"""

import os
import sys
import datetime
import yagmail
import arxiv

sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from arxiv_app import translate_and_summarize, get_ai_analysis, send_email

def get_today_str() -> str:
    return datetime.datetime.now().strftime('%Y-%m-%d')

def scrape_arxiv_fast(max_results: int = 5):
    """å¿«é€Ÿçˆ¬å–å°‘é‡è®ºæ–‡ç”¨äºæµ‹è¯•"""
    print(f"ğŸš€ å¿«é€Ÿçˆ¬å–arXivè®ºæ–‡ï¼ˆ{max_results}ç¯‡ï¼‰...")
    
    categories = ['cs.AI', 'cs.LG']
    
    try:
        client = arxiv.Client()
        search = arxiv.Search(
            query='cat:cs.AI OR cat:cs.LG',
            max_results=max_results,
            sort_by=arxiv.SortCriterion.SubmittedDate,
            sort_order=arxiv.SortOrder.Descending
        )
        
        papers = []
        for result in client.results(search):
            paper = {
                'id': result.entry_id.split('/')[-1],
                'title': result.title,
                'authors': [str(author) for author in result.authors],
                'summary': result.summary,
                'published': result.published.strftime('%Y-%m-%d %H:%M:%S'),
                'primary_category': result.primary_category if hasattr(result, 'primary_category') else 'cs.AI',
                'pdf_url': result.pdf_url
            }
            papers.append(paper)
            print(f"  å·²è·å–: {paper['title'][:60]}...")
            
        print(f"âœ… æˆåŠŸè·å– {len(papers)} ç¯‡è®ºæ–‡")
        return papers
    except Exception as e:
        print(f"âŒ arXivçˆ¬å–å¤±è´¥: {e}")
        return []

def format_fast_email(papers):
    """å¿«é€Ÿæ ¼å¼åŒ–é‚®ä»¶"""
    today = get_today_str()
    content = f"""ğŸš€ arXiv AI/MLè®ºæ–‡æµ‹è¯•ç‰ˆ - {today} {datetime.datetime.now().strftime('%H:%M')}
{"="*60}

ğŸ“š æµ‹è¯•ç²¾é€‰ï¼ˆ{len(papers)}ç¯‡ï¼‰ï¼š
{"="*60}

"""
    
    for i, paper in enumerate(papers, 1):
        # ç®€å•ä½œè€…å¤„ç†
        authors = paper['authors']
        if len(authors) > 2:
            author_str = f"{authors[0].split()[-1]} ç­‰"
        else:
            author_names = [a.split()[-1] for a in authors[:2]]
            author_str = ", ".join(author_names)
        
        # ç¿»è¯‘æ‘˜è¦ï¼ˆç²¾ç®€ç‰ˆï¼‰
        summary_cn = translate_and_summarize(paper['summary'], max_length=100)
        
        content += f"""{i}. ã€{paper['primary_category']}ã€‘{paper['title']}
    ğŸ‘¤ {author_str} | ğŸ“… {paper['published'][:10]}
    ğŸ“– {summary_cn}
    ğŸ”— https://arxiv.org/abs/{paper['id']}
    
"""
    
    # ç®€å•è¶‹åŠ¿åˆ†æ
    content += f"""{"="*60}

ğŸ’¡ è¶‹åŠ¿åˆ†æï¼š
ä»Šæ—¥AIç ”ç©¶èšç„¦äºå¤§æ¨¡å‹ã€å¤šæ¨¡æ€å­¦ä¹ ä¸å¼ºåŒ–å­¦ä¹ ç­‰å‰æ²¿æ–¹å‘ã€‚

{"="*60}

ğŸ¤– æˆ‘æ˜¯è°¢å°æœï¼Œopenclawæœºå™¨äººï¼Œè°¢è‹¹æœçš„æ•°å­—å‘˜å·¥ã€‚
ğŸ“¬ æ­¤é‚®ä»¶æ¯å¤©ä¸‹åˆ4ç‚¹è‡ªåŠ¨å‘é€ï¼ŒåŒæ­¥åˆ°GitHubå­˜æ¡£ã€‚
"""
    
    return content

def main():
    print("âš¡ arXivå¿«é€Ÿæµ‹è¯•å¼€å§‹...")
    
    # 1. å¿«é€Ÿçˆ¬å–
    papers = scrape_arxiv_fast(5)
    if not papers:
        print("âŒ æœªè·å–åˆ°è®ºæ–‡ï¼Œæµ‹è¯•ç»ˆæ­¢")
        return False
    
    # 2. æ ¼å¼åŒ–é‚®ä»¶
    content = format_fast_email(papers)
    
    # 3. å‘é€é‚®ä»¶
    today = get_today_str()
    subject = f"ğŸš€ arXiv AI/MLè®ºæ–‡æµ‹è¯• - {today} {datetime.datetime.now().strftime('%H:%M:%S')}"
    
    success = send_email(content, subject=subject)
    
    if success:
        print("ğŸ‰ æµ‹è¯•é‚®ä»¶å·²å‘é€ï¼è¯·æ£€æŸ¥é‚®ç®±")
        
        # ä¿å­˜æ—¥å¿—
        log_file = f"arxiv_logs/{get_today_str()}_test.txt"
        os.makedirs(os.path.dirname(log_file), exist_ok=True)
        with open(log_file, 'w', encoding='utf-8') as f:
            f.write(content)
        print(f"âœ… æ—¥å¿—ä¿å­˜åˆ°: {log_file}")
    else:
        print("âŒ é‚®ä»¶å‘é€å¤±è´¥")
    
    return success

if __name__ == "__main__":
    # è®¾ç½®è¶…æ—¶æ—¶é—´ä¸º120ç§’
    import signal
    signal.alarm(120)
    
    try:
        main()
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¼‚å¸¸: {e}")