import os
import yagmail
import datetime
import codecs
import requests
import schedule
import time
from zhipuai import ZhipuAI
from pyquery import PyQuery as pq


def get_contents(path):
    with open(path, 'r', encoding='utf-8') as f:
        return f.read()

def get_emails(path):
    with open(path, 'r') as f:
        return f.read().splitlines()

def get_ai_analysis(path):
    try:
        client = ZhipuAI(api_key=os.environ.get("ZHIPUAI_API_KEY"))
        deals = get_contents(path)
        print(f'ai is reading, the info is:\n{deals[:500]}...')

        response = client.chat.completions.create(
            model="glm-4-flash",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªAIä¼˜æƒ ä¿¡æ¯åˆ†æä¸“å®¶ã€‚è´Ÿè´£åˆ†ææ¥è‡ªHacker Newsã€GitHubã€V2EXã€å›½å†…AIå¹³å°çš„ä¼˜æƒ ä¿¡æ¯ã€‚ç­›é€‰å‡ºçœŸå®æœ‰æ•ˆçš„ä¼˜æƒ ï¼Œè¿‡æ»¤æ‰æ— å…³ä¿¡æ¯ã€‚å°†å†…å®¹ç¿»è¯‘æˆä¸­æ–‡ï¼Œè¾“å‡ºæ•´é½ç²¾è‡´çš„HTMLæ ¼å¼ã€‚ä½¿ç”¨HTMLæ ‡ç­¾ï¼š<h2>ã€<h3>ã€<ul>ã€<li>ã€<a>ã€<strong>ç­‰ã€‚é‡ç‚¹æ ‡æ³¨ï¼š(1)å“ªäº›å¹³å°/äº§å“æœ‰ä¼˜æƒ  (2)ä¼˜æƒ åŠ›åº¦ (3)å¦‚ä½•è·å– (4)æˆªæ­¢æ—¶é—´ã€‚æŒ‰ä¼˜æƒ åŠ›åº¦æ’åºï¼Œæ¨è3ä¸ªæœ€å€¼å¾—è–…ç¾Šæ¯›çš„æœºä¼šã€‚è¯­è¨€ç®€æ´å®ç”¨ã€‚æœ€åä¸€å¥è¯ï¼šæˆ‘æ˜¯è°¢è‹¹æœï¼ŒAIä¿¡æ¯æµ2.0ï¼Œè®©ä½ çš„tokenç”¨ä¸å®Œã€‚"},
                {"role": "user", "content": f'{deals}'}
            ],
        )

        ans = response.choices[0].message.content
        return ans
    except Exception as e:
        print(f'when ai analyze, {e} occurs...')
        return None


def convert_to_html_email(content):
    """
    å°†AIç”Ÿæˆçš„å†…å®¹è½¬æ¢ä¸ºç¾è§‚çš„HTMLé‚®ä»¶æ ¼å¼
    """
    # å»æ‰å¯èƒ½å­˜åœ¨çš„```htmlæ ‡ç­¾
    content = content.strip()
    if content.startswith('```html'):
        content = content[7:]  # å»æ‰å¼€å¤´çš„```html
    if content.endswith('```'):
        content = content[:-3]  # å»æ‰ç»“å°¾çš„```
    content = content.strip()

    # å¦‚æœAIå·²ç»è¿”å›HTMLï¼Œç›´æ¥ä½¿ç”¨
    if '<html>' in content or '<h2>' in content:
        html_content = content
    else:
        # å¦åˆ™è¿›è¡Œç®€å•çš„Markdownåˆ°HTMLè½¬æ¢
        html_content = content.replace('\n', '<br>')

    # åŒ…è£…åœ¨é‚®ä»¶æ¨¡æ¿ä¸­
    html_email = f"""
    <!DOCTYPE html>
    <html>
    <head>
        <meta charset="UTF-8">
        <style>
            body {{
                font-family: 'Microsoft YaHei', Arial, sans-serif;
                line-height: 1.6;
                color: #333;
                max-width: 800px;
                margin: 0 auto;
                padding: 20px;
                background-color: #f5f5f5;
            }}
            .container {{
                background-color: white;
                padding: 30px;
                border-radius: 10px;
                box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            }}
            h1 {{
                color: #2c3e50;
                border-bottom: 3px solid #3498db;
                padding-bottom: 10px;
            }}
            h2 {{
                color: #3498db;
                margin-top: 30px;
                border-left: 4px solid #3498db;
                padding-left: 10px;
            }}
            h3 {{
                color: #e74c3c;
                margin-top: 20px;
            }}
            ul {{
                list-style-type: none;
                padding-left: 0;
            }}
            li {{
                background-color: #ecf0f1;
                margin: 10px 0;
                padding: 15px;
                border-radius: 5px;
                border-left: 4px solid #3498db;
            }}
            a {{
                color: #3498db;
                text-decoration: none;
            }}
            a:hover {{
                text-decoration: underline;
            }}
            .highlight {{
                background-color: #fff3cd;
                padding: 15px;
                border-radius: 5px;
                border-left: 4px solid #ffc107;
                margin: 20px 0;
            }}
            .footer {{
                margin-top: 30px;
                padding-top: 20px;
                border-top: 2px solid #ecf0f1;
                text-align: center;
                color: #7f8c8d;
                font-size: 14px;
            }}
            strong {{
                color: #e74c3c;
            }}
        </style>
    </head>
    <body>
        <div class="container">
            <h1>ğŸ‰ ä»Šæ—¥AIè–…ç¾Šæ¯›å…¨ç½‘æ±‡æ€»</h1>
            {html_content}
            <div class="footer">
                <p>ğŸ“§ è¿™æ˜¯ä¸€å°è‡ªåŠ¨ç”Ÿæˆçš„é‚®ä»¶</p>
                <p>å¦‚æœ‰é—®é¢˜ï¼Œè¯·è”ç³»ï¼š19121220286@163.com</p>
            </div>
        </div>
    </body>
    </html>
    """
    return html_email


def createtext(filename):
    with open(filename, 'w', encoding='utf-8') as f:
        f.write('')


def scrape_hackernews():
    """
    çˆ¬å–Hacker Newsä¸ŠAIç›¸å…³çš„ä¼˜æƒ ä¿¡æ¯
    """
    print("\n=== å¼€å§‹çˆ¬å– Hacker News ===")
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }

        search_queries = [
            'AI free', 'GPT free', 'LLM free', 'AI API free',
            'ChatGPT deal', 'AI launch', 'AI beta', 'OpenAI credit',
            'Claude free', 'AI token', 'AI discount', 'AI promo'
        ]

        all_posts = []

        for query in search_queries:
            try:
                url = f'https://hn.algolia.com/api/v1/search?query={query}&tags=story&numericFilters=created_at_i>{int(time.time()) - 7*24*3600}'
                response = requests.get(url, headers=headers, timeout=10)

                if response.status_code == 200:
                    data = response.json()
                    hits = data.get('hits', [])

                    for hit in hits:
                        # å®‰å…¨è·å–objectID
                        object_id = hit.get('objectID', '')
                        if not object_id:
                            continue

                        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ï¼ˆä½¿ç”¨objectIDå»é‡ï¼‰
                        if any(p.get('objectID') == object_id for p in all_posts):
                            continue

                        title = hit.get('title', '').lower()
                        ai_keywords = ['ai', 'gpt', 'llm', 'chatgpt', 'claude', 'openai', 'model', 'api']
                        deal_keywords = ['free', 'deal', 'discount', 'promo', 'credit', 'trial', 'launch', 'beta']

                        has_ai = any(kw in title for kw in ai_keywords)
                        has_deal = any(kw in title for kw in deal_keywords)

                        if has_ai and has_deal:
                            all_posts.append({
                                'source': 'Hacker News',
                                'title': hit.get('title', ''),
                                'url': hit.get('url', f"https://news.ycombinator.com/item?id={object_id}"),
                                'score': hit.get('points', 0),
                                'objectID': object_id,  # ä¿å­˜objectIDç”¨äºå»é‡
                            })

                time.sleep(1)
            except Exception as e:
                print(f'Error searching HN for {query}: {e}')
                continue

        all_posts.sort(key=lambda x: x['score'], reverse=True)
        print(f"Hacker News: æ‰¾åˆ° {len(all_posts[:15])} æ¡ç›¸å…³ä¿¡æ¯")
        return all_posts[:15]

    except Exception as e:
        print(f'Error in scrape_hackernews: {e}')
        return []


def scrape_github_awesome():
    """
    çˆ¬å–GitHubä¸Šçš„Awesomeåˆ—è¡¨ï¼Œå¯»æ‰¾å…è´¹AIèµ„æº
    """
    print("\n=== å¼€å§‹çˆ¬å– GitHub Awesomeåˆ—è¡¨ ===")
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }

        # çŸ¥åçš„AIèµ„æºåˆ—è¡¨
        awesome_repos = [
            'LiLittleCat/awesome-free-chatgpt',
            'sindresorhus/awesome-chatgpt',
            'f/awesome-chatgpt-prompts',
            'humanloop/awesome-chatgpt',
        ]

        all_items = []

        for repo in awesome_repos:
            try:
                # è·å–READMEå†…å®¹
                url = f'https://api.github.com/repos/{repo}/readme'
                response = requests.get(url, headers=headers, timeout=10)

                if response.status_code == 200:
                    # ä¹Ÿå¯ä»¥ç›´æ¥çˆ¬å–ç½‘é¡µç‰ˆ
                    readme_url = f'https://github.com/{repo}'
                    page_response = requests.get(readme_url, headers=headers, timeout=10)

                    if page_response.status_code == 200:
                        d = pq(page_response.content)
                        # æŸ¥æ‰¾åŒ…å«free, api, tokenç­‰å…³é”®è¯çš„é“¾æ¥
                        links = d('article a')

                        for link in links:
                            link_elem = pq(link)
                            text = link_elem.text().lower()
                            href = link_elem.attr('href')

                            if any(kw in text for kw in ['free', 'api', 'token', 'credit', 'trial', 'å…è´¹']):
                                all_items.append({
                                    'source': f'GitHub/{repo.split("/")[1]}',
                                    'title': link_elem.text(),
                                    'url': href if href.startswith('http') else f'https://github.com{href}',
                                    'score': 0,
                                })

                print(f"GitHub {repo}: å®Œæˆ")
                time.sleep(2)
            except Exception as e:
                print(f'Error scraping {repo}: {e}')
                continue

        print(f"GitHub Awesome: æ‰¾åˆ° {len(all_items[:10])} æ¡ç›¸å…³ä¿¡æ¯")
        return all_items[:10]

    except Exception as e:
        print(f'Error in scrape_github_awesome: {e}')
        return []


def scrape_v2ex():
    """
    çˆ¬å–V2EXçš„AIç›¸å…³èŠ‚ç‚¹
    """
    print("\n=== å¼€å§‹çˆ¬å– V2EX ===")
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }

        # V2EXçš„AIç›¸å…³èŠ‚ç‚¹
        nodes = ['ai', 'chatgpt', 'programmer', 'create']
        all_posts = []

        for node in nodes:
            try:
                url = f'https://www.v2ex.com/api/topics/hot.json'
                response = requests.get(url, headers=headers, timeout=10)

                if response.status_code == 200:
                    topics = response.json()

                    for topic in topics:
                        title = topic.get('title', '').lower()
                        content = topic.get('content', '').lower()

                        # æ£€æŸ¥æ˜¯å¦åŒ…å«AIå’Œä¼˜æƒ å…³é”®è¯
                        ai_keywords = ['ai', 'gpt', 'chatgpt', 'claude', 'openai', 'llm', 'å¤§æ¨¡å‹', 'api']
                        deal_keywords = ['free', 'deal', 'discount', 'promo', 'credit', 'trial', 'å…è´¹', 'ä¼˜æƒ ', 'æŠ˜æ‰£', 'é€']

                        has_ai = any(kw in title or kw in content for kw in ai_keywords)
                        has_deal = any(kw in title or kw in content for kw in deal_keywords)

                        if has_ai and has_deal:
                            all_posts.append({
                                'source': 'V2EX',
                                'title': topic.get('title', ''),
                                'url': f"https://www.v2ex.com/t/{topic.get('id', '')}",
                                'score': topic.get('replies', 0),
                            })

                time.sleep(2)
            except Exception as e:
                print(f'Error scraping V2EX node {node}: {e}')
                continue

        all_posts.sort(key=lambda x: x['score'], reverse=True)
        print(f"V2EX: æ‰¾åˆ° {len(all_posts[:10])} æ¡ç›¸å…³ä¿¡æ¯")
        return all_posts[:10]

    except Exception as e:
        print(f'Error in scrape_v2ex: {e}')
        return []


def scrape_all_sources(filename):
    """
    æ•´åˆæ‰€æœ‰ä¿¡æ¯æº
    """
    try:
        all_deals = []

        # 1. Hacker News
        hn_deals = scrape_hackernews()
        all_deals.extend(hn_deals)

        # 2. GitHub Awesome
        github_deals = scrape_github_awesome()
        all_deals.extend(github_deals)

        # 3. V2EX
        v2ex_deals = scrape_v2ex()
        all_deals.extend(v2ex_deals)

        # å†™å…¥æ–‡ä»¶
        with codecs.open(filename, "w", "utf-8") as f:
            f.write(f"=== AIä¼˜æƒ ä¿¡æ¯å…¨ç½‘æ±‡æ€» ({datetime.datetime.now().strftime('%Y-%m-%d')}) ===\n\n")

            # æŒ‰æ¥æºåˆ†ç»„
            sources = {}
            for deal in all_deals:
                source = deal['source']
                if source not in sources:
                    sources[source] = []
                sources[source].append(deal)

            # è¾“å‡ºæ¯ä¸ªæ¥æºçš„ä¿¡æ¯
            for source, deals in sources.items():
                f.write(f"\n## {source} ({len(deals)}æ¡)\n\n")
                for index, deal in enumerate(deals, start=1):
                    f.write(f"{index}. {deal['title']}\n")
                    f.write(f"   é“¾æ¥: {deal['url']}\n")
                    if deal['score'] > 0:
                        f.write(f"   çƒ­åº¦: {deal['score']}\n")
                    f.write("\n")

        print(f'\næ€»è®¡æ‰¾åˆ° {len(all_deals)} æ¡AIä¼˜æƒ ä¿¡æ¯')
        return len(all_deals) > 0

    except Exception as e:
        print(f'Error in scrape_all_sources: {e}')
        return False


def job():
    strdate = datetime.datetime.now().strftime('%Y-%m-%d')
    os.makedirs('logs', exist_ok=True)
    filename = f'logs/ai_deals_all_{strdate}.txt'
    print(f'{strdate} å¼€å§‹AIä¼˜æƒ å…¨ç½‘çˆ¬å–ä»»åŠ¡...')
    createtext(filename)

    attempts = 0

    while attempts < 3:  # å‡å°‘é‡è¯•æ¬¡æ•°ï¼Œå› ä¸ºæœ‰å¤šä¸ªæº
        try:
            print(f'ç¬¬ {attempts + 1} æ¬¡å°è¯•çˆ¬å–...')
            success = scrape_all_sources(filename)

            if not success:
                raise Exception("No deals found")

            print('\nçˆ¬å–å®Œæˆï¼Œå¼€å§‹AIåˆ†æ...\n')

            ans = get_ai_analysis(filename)
            if ans:
                print(f'AIåˆ†æç»“æœ:\n{ans}\n')

                # ä¿å­˜AIåˆ†æç»“æœ
                with open(filename, 'w', encoding='utf-8') as f:
                    f.write(ans)
                return filename
            else:
                raise Exception("AI analysis failed")

        except Exception as e:
            attempts += 1
            print(f"ç¬¬ {attempts} æ¬¡å°è¯•å¤±è´¥: {e}")
            if attempts < 3:
                time.sleep(180)  # ç­‰å¾…3åˆ†é’Ÿåé‡è¯•

    raise Exception("æ‰€æœ‰å°è¯•å‡å¤±è´¥")


def send_email(src, dst, subject, contents, attachments=None):
    pwd = os.environ.get('wangyi_emai_auth')
    yag = yagmail.SMTP(user=src, password=pwd, host='smtp.163.com', port='465')
    yag.send(to=dst, subject=subject, contents=contents, attachments=attachments)
    yag.close()

def send_emails(src, tos, subject, contents, attachments=None):
    for to in tos:
        send_email(src, to, subject, contents, attachments)

def daily_task():
    try:
        path = job()
        src = '19121220286@163.com'
        tos = get_emails('emails_deals.txt')
        subject = 'ğŸ‰ ä»Šæ—¥AIè–…ç¾Šæ¯›å…¨ç½‘æ±‡æ€»'

        # è¯»å–AIåˆ†æç»“æœå¹¶è½¬æ¢ä¸ºHTML
        content = get_contents(path)
        html_content = convert_to_html_email(content)

        # yagmailä¼šè‡ªåŠ¨è¯†åˆ«HTMLå†…å®¹
        send_emails(src, tos, subject, html_content, None)  # ä¸éœ€è¦é™„ä»¶
        print("é‚®ä»¶å‘é€å®Œæˆï¼")
    except Exception as e:
        print(f"daily_taskå‡ºé”™: {e}")

if __name__ == '__main__':
    try:
        schedule.every().day.at('12:10').do(daily_task)

        while True:
            schedule.run_pending()
            time.sleep(1)

    except Exception as e:
        print(f"ç¨‹åºå‡ºé”™: {e}")

