import os
import yagmail
import datetime
import codecs
import requests
import schedule
import time
import socket
from pyquery import PyQuery as pq
from zhipuai import ZhipuAI

# 网络优化配置
socket.setdefaulttimeout(45)  # 全局socket超时45秒


def get_contents(path):
    with open(path, 'r', encoding='utf-8') as f:
        return f.read()

def get_emails(path):
    with open(path, 'r') as f:
        return f.read().splitlines()

def get_ai_analysis(path):
    try:
        client = ZhipuAI(api_key=os.environ.get("ZHIPUAI_API_KEY"))    
        trends = get_contents(path)
        print(f'ai is reading, the info is:\n{trends}')

        response = client.chat.completions.create(
            model="glm-4-flash",  # 填写需要调用的模型编码
            messages=[
                {"role": "system", "content": """你是一个专业的 GitHub 趋势分析专家，负责分析每日 Python 项目的趋势。

## 你的任务：
1. **安全筛选**：首先提取安全和不敏感的项目
2. **深度分析**：为每个项目提供以下信息：
   - 中文翻译和简介
   - 项目分类标签（从以下标签选择：视觉AI、开发者工具、AI平台、创新应用、基础设施工具、Web3/区块链、数据分析、机器学习框架）
   - 技术亮点（1-2个关键技术创新点）
   - 潜在应用场景

3. **趋势洞察**：
   - 选出1个最惊艳的项目，详细说明为什么惊艳
   - 识别今日的技术趋势主题（如"隐私友好AI感知技术兴起"、"低代码AI平台爆发"等）
   - 分析投资/关注热点：哪些领域项目数量增多
   - 预测下一个可能的热门方向

4. **输出格式**：
   ## 今日GitHub趋势分析报告
   
   ### 最惊艳项目
   [项目名称] - [一句话描述]
   为什么惊艳：[详细解释]
   
   ### 项目分类概览
   - **视觉AI**：[项目数量]个，如：[项目1]、[项目2]
   - **开发者工具**：[项目数量]个，如：[项目1]、[项目2]
   - **AI平台**：[项目数量]个，如：[项目1]、[项目2]
   - [其他分类]...
   
   ### 今日技术趋势
   [识别的主要趋势主题，如：隐私友好AI感知技术兴起]
   
   ### 深度洞察
   1. [洞察点1：如哪些领域增长最快]
   2. [洞察点2：如技术创新模式分析]
   3. [洞察点3：如潜在商业机会]
   
   ### 预测建议
   [基于今日趋势，下一个可能爆发的方向]
   
   我是谢苹果，AI信息流2.0，由nanobot智能优化，复活了。"""},
                {"role": "user", "content":f'{trends}' }
            ],
        )

        ans = response.choices[0].message.content
        return ans
    except Exception as e:
        print(f'when ai analyze, {e} occurs...')


def createtext(filename):
    with open(filename, 'w') as f:
        f.write('')


def scrape(language, filename):
    try:
        HEADERS = {
            'User-Agent'		: 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept'			: 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
            'Accept-Encoding'	: 'gzip, deflate, br',
            'Accept-Language'	: 'zh-CN,zh;q=0.9,en;q=0.8',
            'Connection'		: 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1'
        }

        url = f'https://github.com/trending/{language}'
        r = requests.get(url, headers=HEADERS, timeout=45)  # 增加超时到45秒，匹配全局设置

        print(f'scrape() status_code: {r.status_code}')
        assert r.status_code == 200

        # print(f'scrape() content:\n {r.content}')        
        d = pq(r.content)
        items = d('div.Box article.Box-row')


        with codecs.open(filename, "a", "utf-8") as f:
            for index, item in enumerate(items, start=1):
                i = pq(item)
                title = i(".lh-condensed a").text()
                description = i("p.col-9").text()
                url = i(".lh-condensed a").attr("href")
                url = "https://github.com" + url
                print(url)
                f.write(u"{index}. [{title}]:{description}({url})\n".format(index=index, title=title, description=description, url=url))
    except Exception as e:
        print(f'when scrape {e} occurs')

def job():
    strdate = datetime.datetime.now().strftime('%Y-%m-%d')
    os.makedirs('logs', exist_ok=True)
    filename = f'logs/{strdate}.txt'
    print(f'{strdate} start the job...')
    createtext(filename)

    attempts = 0

    while attempts < 6:
        try:
            print(f'try at {attempts} to get trends~')
            scrape('python', filename)   # weak point
            print('scape ends\n')

            ans = get_ai_analysis(filename)
            print(f'ai ans is:\n {ans}')
            
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(ans)
            return filename
        except Exception as e:
            attempts += 1
            print(f"Attempt {attempts} failed with error: {e}")
            time.sleep(300)  # Wait 300s before retrying
    raise Exception("All attempts to scrape data have failed.")



def send_email(src, dst, subject, contents, attachments):
    pwd = os.environ.get('wangyi_emai_auth')

    yag = yagmail.SMTP(user=src, password=pwd, host='smtp.163.com', port='465')
    yag.send(to=dst, subject=subject, contents=contents, attachments=attachments)
    yag.close()

def send_emails(src, tos, subject, contents, attachments):
    for to in tos:
        send_email(src, to, subject, contents, attachments)  

def daily_task():
    try:
        path = job()
        src = '19121220286@163.com'
        tos = get_emails('emails.txt') 
        subject = '今日AI+头条项目'
        contents = get_contents(path)
        attachments = path
        
        send_emails(src, tos, subject, contents, attachments)
    except Exception as e:
        print(f"{e} occured in daily_task")

if __name__ == '__main__':
    try:
        schedule.every().day.at('21:00').do(daily_task)

        while True:
            schedule.run_pending()
            time.sleep(1)

    except Exception as e:
        print(f"{e} occured~")